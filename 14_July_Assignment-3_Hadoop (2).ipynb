{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7b7e1ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "Question 1. Write a Python program to read a Hadoop configuration file and display the core components of Hadoop.\n",
    "Solution:- # Python program to read a Hadoop configuration file\n",
    "    \n",
    "    Code\n",
    "    \n",
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "\"\"\"\n",
    "    Read and get your Hadoop configuration as Python objects.\n",
    "    For example: core-site.xml, yarn-site.xml, hdfs-site.xml\n",
    "\"\"\"\n",
    "\n",
    "from .core import get_hadoop_conf\n",
    "\n",
    "__version__ = \"1.0\"\n",
    "\n",
    "# Python program to  display the core components of Hadoop\n",
    "\n",
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "\"\"\"\n",
    "    Implementation\n",
    "\n",
    "    Usage:\n",
    "\n",
    "    >>> from hadoopconf import get_hadoop_conf\n",
    "    >>> get_hadoop_conf()\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "#  String list of what we can import from the lib\n",
    "import os\n",
    "import re\n",
    "import xml.etree.ElementTree as Et\n",
    "\n",
    "__all__ = ['get_hadoop_conf']\n",
    "\n",
    "\n",
    "class NoHadoopConfDir(Exception):\n",
    "    pass\n",
    "\n",
    "\n",
    "def get_conf_dir(dirname=None):\n",
    "    \"\"\"\n",
    "    Return the Hadoop configuration directory absolute path. Raise a NoHadoopConfDir exception if\n",
    "    no environment variable defined it (HADOOP_CONF_DIR or HADOOP_HOME) or if dirname is not a valid directory path\n",
    "    :param dirname: Optional absolute path of a directory that should contains *-site.xml files\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    if dirname:\n",
    "        if os.path.exists(dirname) and os.path.isdir(dirname):\n",
    "            return dirname\n",
    "        else:\n",
    "            NoHadoopConfDir()\n",
    "    if 'HADOOP_CONF_DIR' in os.environ:\n",
    "        return os.environ['HADOOP_CONF_DIR']\n",
    "    if 'HADOOP_HOME' in os.environ:\n",
    "        dirname = os.path.join(os.environ['HADOOP_HOME'], 'etc', 'hadoop')\n",
    "        if os.path.exists(dirname) and os.path.isdir(dirname):\n",
    "            return dirname\n",
    "        else:\n",
    "            NoHadoopConfDir()\n",
    "    else:\n",
    "        raise NoHadoopConfDir()\n",
    "\n",
    "\n",
    "def get_conf_files(dirname):\n",
    "    \"\"\"\n",
    "    Generator of all *-site.xml files in a directory\n",
    "    :param dirname: absolute path of a directory that should contains *-site.xml files\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    for file in os.listdir(dirname):\n",
    "        if re.match(r'.+-site\\.xml', file):\n",
    "            yield os.path.join(dirname, file)\n",
    "\n",
    "\n",
    "def parse_file(file):\n",
    "    \"\"\"\n",
    "    Parse an xml Hadoop property file\n",
    "    :param file: absolute path\n",
    "    :return: a dictionary (propertyName -> propertyValueInString)\n",
    "    \"\"\"\n",
    "    tree = Et.parse(file)\n",
    "    root = tree.getroot()\n",
    "    result = dict()\n",
    "    for child in root.findall('property'):  # Find all properties names an values\n",
    "        result[child.find('name').text] = child.find('value').text\n",
    "    return result\n",
    "\n",
    "\n",
    "def get_hadoop_conf(dirname=None):\n",
    "    \"\"\"\n",
    "    Return a dict of all properties find in all *-site.xml files on your Hadoop configuration directory.\n",
    "    It will search in your env variable HADOOP_CONF_DIR next in HADOOP_HOME/etc/hadoop or in dirname if you specify it\n",
    "    :param dirname: optional directory absolute path that contains *-site.xml file\n",
    "    :return: a dictionary (propertyName -> propertyValueInString)\n",
    "    \"\"\"\n",
    "    dirname = get_conf_dir(dirname)\n",
    "    files = get_conf_files(dirname)\n",
    "    all_conf = dict()\n",
    "    for file in files:\n",
    "        all_conf.update(parse_file(file))  # Merge dictionnaries\n",
    "    return all_conf\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    conf = get_hadoop_conf()\n",
    "    print(conf['yarn.resourcemanager.webapp.address'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2203ac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "Question 2. Implement a Python function that calculates the total file size in a Hadoop Distributed File System (HDFS) directory.\n",
    "\n",
    "Solution:- import subprocess\n",
    "import sys, getopt\n",
    "import re\n",
    "\n",
    "# Dictionnary to store the total size by ip node\n",
    "size_by_ip_dict = {}\n",
    "\n",
    "# Print size in a human readable way\n",
    "def human_readable_size(size):\n",
    "    for unit in ['','K','M','G','T','P','E','Z']:\n",
    "        if abs(size) < 1024.0:\n",
    "            return \"%3.1f%s%s\" % (size, unit, ' B')\n",
    "        size /= 1024.0\n",
    "    return \"%.1f%s%s\" % (size, 'Y', ' B') #Should never happen\n",
    "\n",
    "# Sum the given block_size to the ip entry in the size_by_ip_dict\n",
    "def update_size_by_ip_dict(ip, block_size):\n",
    "    if ip not in size_by_ip_dict.keys(): \n",
    "        size_by_ip_dict[ip] = block_size\n",
    "    else:\n",
    "        size_by_ip_dict[ip] += block_size\n",
    "\n",
    "def main(argv):\n",
    "    path = sys.argv[1]\n",
    "\n",
    "    # Call the hdfs fsck command\n",
    "    out = subprocess.Popen(['hdfs', 'fsck', path, '-files' ,'-blocks', '-locations'], stdout=subprocess.PIPE, stderr=subprocess.STDOUT)\n",
    "    stdout,stderr = out.communicate()\n",
    "    lines = stdout.decode(\"utf-8\").split(\"\\n\")\n",
    "    # Filter the lines corresponding to a HDFS block information\n",
    "    hdfs_block_lines = [line for line in lines if \"DatanodeInfoWithStorage\" in line]\n",
    "\n",
    "    # Regex to get the length and replication factor of a HDFS block\n",
    "    regex_len_rep = r'.* len=(\\d*) repl=(\\d*).*'\n",
    "\n",
    "    for hdfs_block_line in hdfs_block_lines:\n",
    "        match_object_len_rep = re.match(regex_len_rep, hdfs_block_line)\n",
    "        block_size = int(match_object_len_rep.group(1))\n",
    "        replication_factor = int(match_object_len_rep.group(2))\n",
    "        # Regex to get all the ips on which a HDFS block is present\n",
    "        regex_ips = r'.*' + '.*DatanodeInfoWithStorage\\[([\\w.]*)' * replication_factor\n",
    "        match_object_ips = re.match(regex_ips, hdfs_block_line)\n",
    "        for i in range(0, replication_factor): \n",
    "            ip = match_object_ips.group(i + 1)\n",
    "            update_size_by_ip_dict(ip, block_size)\n",
    "\n",
    "    total_size = 0\n",
    "    # Calculate the total size\n",
    "    for ip, size in size_by_ip_dict.items():\n",
    "        total_size += size\n",
    "\n",
    "    print(\"Total size = \", human_readable_size(total_size))\n",
    "    print(\"Data by node\")\n",
    "    sorted_size_by_ip_list = sorted(size_by_ip_dict.items(), key=lambda x: x[1])\n",
    "    # Print the size by node sorted by size\n",
    "    for ip, size in sorted_size_by_ip_list:\n",
    "        print(ip, \":\", human_readable_size(size), \"(\", \"%.2f\" % (size / total_size * 100), \"%)\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "   main(sys.argv[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6308f4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "Question 3. Create a Python program that extracts and displays the top N most frequent words from a large text file using the MapReduce approach.\n",
    "\n",
    "Solution:- mapper.py\n",
    "    \n",
    "    #!/usr/bin/python\n",
    "  \n",
    "# import sys because we need to read and write data to STDIN and STDOUT\n",
    "import sys\n",
    "  \n",
    "# reading entire line from STDIN (standard input)\n",
    "for line in sys.stdin:\n",
    "    # to remove leading and trailing whitespace\n",
    "    line = line.strip()\n",
    "    # split the line into words\n",
    "    words = line.split()\n",
    "      \n",
    "    # we are looping over the words array and printing the word\n",
    "    # with the count of 1 to the STDOUT\n",
    "    for word in words:\n",
    "        # write the results to STDOUT (standard output);\n",
    "        # what we output here will be the input for the\n",
    "        # Reduce step, i.e. the input for reducer.py\n",
    "        print('%s\\t%s' % (word, 1))\n",
    "        \n",
    "        \n",
    "reducer.py\n",
    "\n",
    "#!/usr/bin/python\n",
    "  \n",
    "from operator import itemgetter\n",
    "import sys\n",
    "  \n",
    "current_word = None\n",
    "current_count = 0\n",
    "word = None\n",
    "  \n",
    "# read the entire line from STDIN\n",
    "for line in sys.stdin:\n",
    "    # remove leading and trailing whitespace\n",
    "    line = line.strip()\n",
    "    # slpiting the data on the basis of tab we have provided in mapper.py\n",
    "    word, count = line.split('\\t', 1)\n",
    "    # convert count (currently a string) to int\n",
    "    try:\n",
    "        count = int(count)\n",
    "    except ValueError:\n",
    "        # count was not a number, so silently\n",
    "        # ignore/discard this line\n",
    "        continue\n",
    "  \n",
    "    # this IF-switch only works because Hadoop sorts map output\n",
    "    # by key (here: word) before it is passed to the reducer\n",
    "    if current_word == word:\n",
    "        current_count += count\n",
    "    else:\n",
    "        if current_word:\n",
    "            # write result to STDOUT\n",
    "            print('%s\\t%s' % (current_word, current_count))\n",
    "        current_count = count\n",
    "        current_word = word\n",
    "  \n",
    "# do not forget to output the last word if needed!\n",
    "if current_word == word:\n",
    "    print('%s\\t%s' % (current_word, current_count))\n",
    "    \n",
    "\n",
    "demo.txt\n",
    "\n",
    "hello hi bye ok\n",
    "ok bye nice hello\n",
    "tata bye bye\n",
    "\n",
    "hadoop-streaming-2.4.0.jar->needed\n",
    "\n",
    "hdfs_commands.txt\n",
    "\n",
    "# In hdfs file system / is the root\n",
    "\n",
    "# Command to check the files inside root hdfs directory\n",
    "hadoop fs -ls /\n",
    "\n",
    "# Command to create directory in hdfs\n",
    "hadoop fs -mkdir /input_data\n",
    "\n",
    "\n",
    "# Copy data from local file system to Hdfs\n",
    "hadoop fs -put test_demo/trees.csv /input_data\n",
    "\n",
    "# Copy from HDFS path to local file system\n",
    "hadoop fs -copyToLocal /input_data/trees.csv ./\n",
    "\n",
    "# Command to execute map reduce code\n",
    "hadoop jar hadoop-streaming-2.4.0.jar -files mapper.py,reducer.py -mapper mapper.py -reducer reducer.py -input /test/demo.txt -output /output\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d28ddd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "Question 4. Write a Python script that checks the health status of the NameNode and DataNodes in a Hadoop cluster using Hadoop's REST API.\n",
    "\n",
    "Solution:- # Python script that checks the health status of the NameNode and DataNodes\n",
    "    #! /usr/bin/python -v\n",
    "\n",
    "import os\n",
    "import subprocess\n",
    "\n",
    "f = os.popen(\"hdfs haadmin -getServiceState nn2\")\n",
    "now = f.read()\n",
    "status = \"active\"\n",
    "if now == status:\n",
    "        print \"success\"\n",
    "else:\n",
    "        print 'error'\n",
    "        \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93f0939a",
   "metadata": {},
   "outputs": [],
   "source": [
    "Question 5. Develop a Python program that lists all the files and directories in a specific HDFS path.\n",
    "\n",
    "Solution:-  hdfs.py\n",
    "    \n",
    "    \"\"\"\n",
    "API for interacting with the file system on Hops (HopsFS).\n",
    "\n",
    "It is a wrapper around pydoop together with utility functions that are Hops-specific.\n",
    "\"\"\"\n",
    "import socket\n",
    "from six import string_types\n",
    "import shutil\n",
    "import fnmatch\n",
    "import os\n",
    "import errno\n",
    "\n",
    "from hops import constants\n",
    "from hops.service_discovery import ServiceDiscovery\n",
    "import sys\n",
    "import subprocess\n",
    "\n",
    "import ntpath\n",
    "import importlib\n",
    "\n",
    "import logging\n",
    "log = logging.getLogger(__name__)\n",
    "\n",
    "# Compatibility with SageMaker\n",
    "pydoop_available = True\n",
    "try:\n",
    "    import pydoop.hdfs as hdfs\n",
    "    import pydoop.hdfs.path as path\n",
    "    import pydoop.hdfs.fs as hdfs_fs\n",
    "except:\n",
    "    pydoop_available = False\n",
    "\n",
    "import re\n",
    "from xml.dom import minidom\n",
    "\n",
    "tls_enabled = None\n",
    "webhdfs_address = None\n",
    "\n",
    "if pydoop_available: \n",
    "    # Replace Pydoop split method to be able to support hopsfs:// schemes\n",
    "    class _HopsFSPathSplitter(hdfs.path._HdfsPathSplitter):\n",
    "        @classmethod\n",
    "        def split(cls, hdfs_path, user):\n",
    "            if not hdfs_path:\n",
    "                cls.raise_bad_path(hdfs_path, \"empty\")\n",
    "            scheme, netloc, path = cls.parse(hdfs_path)\n",
    "            if not scheme:\n",
    "                scheme = \"file\" if hdfs_fs.default_is_local() else \"hdfs\"\n",
    "            if scheme == \"hdfs\" or scheme == \"hopsfs\":\n",
    "                if not path:\n",
    "                    cls.raise_bad_path(hdfs_path, \"path part is empty\")\n",
    "                if \":\" in path:\n",
    "                    cls.raise_bad_path(\n",
    "                        hdfs_path, \"':' not allowed outside netloc part\"\n",
    "                    )\n",
    "                hostname, port = cls.split_netloc(netloc)\n",
    "                if not path.startswith(\"/\"):\n",
    "                    path = \"/user/%s/%s\" % (user, path)\n",
    "            elif scheme == \"file\":\n",
    "                hostname, port, path = \"\", 0, netloc + path\n",
    "            else:\n",
    "                cls.raise_bad_path(hdfs_path, \"unsupported scheme %r\" % scheme)\n",
    "            return hostname, port, path\n",
    "\n",
    "    hdfs.path._HdfsPathSplitter = _HopsFSPathSplitter\n",
    "\n",
    "def get_plain_path(abs_path):\n",
    "    \"\"\"\n",
    "    Convert absolute HDFS/HOPSFS path to plain path (dropping prefix and ip)\n",
    "\n",
    "    Example use-case:\n",
    "\n",
    "    >>> hdfs.get_plain_path(\"hdfs://10.0.2.15:8020/Projects/demo_deep_learning_admin000/Models/\")\n",
    "    >>> # returns: \"/Projects/demo_deep_learning_admin000/Models/\"\n",
    "\n",
    "     Args:\n",
    "         :abs_path: the absolute HDFS/hopsfs path containing prefix and/or ip\n",
    "\n",
    "    Returns:\n",
    "          the plain path without prefix and ip\n",
    "    \"\"\"\n",
    "    return path.split(path.abspath(abs_path))[2]\n",
    "\n",
    "def project_id():\n",
    "    \"\"\"\n",
    "    Get the Hopsworks project id from environment variables\n",
    "\n",
    "    Returns: the Hopsworks project id\n",
    "\n",
    "    \"\"\"\n",
    "    return os.environ[constants.ENV_VARIABLES.HOPSWORKS_PROJECT_ID_ENV_VAR]\n",
    "\n",
    "def project_user():\n",
    "    \"\"\"\n",
    "    Gets the project username (\"project__user\") from environment variables\n",
    "\n",
    "    Returns:\n",
    "        the project username\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        hops_user = os.environ[constants.ENV_VARIABLES.HADOOP_USER_NAME_ENV_VAR]\n",
    "    except:\n",
    "        hops_user = os.environ[constants.ENV_VARIABLES.HDFS_USER_ENV_VAR]\n",
    "    return hops_user\n",
    "\n",
    "def project_name():\n",
    "    \"\"\"\n",
    "    Extracts the project name from the project username (\"project__user\") or from the environment if available\n",
    "\n",
    "    Returns:\n",
    "        project name\n",
    "    \"\"\"\n",
    "    try:\n",
    "        return os.environ[constants.ENV_VARIABLES.HOPSWORKS_PROJECT_NAME_ENV_VAR]\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    hops_user = project_user()\n",
    "    hops_user_split = hops_user.split(\"__\")  # project users have username project__user\n",
    "    project = hops_user_split[0]\n",
    "    return project\n",
    "\n",
    "def project_path(project=None, exclude_nn_addr=False):\n",
    "    \"\"\" Get the path in HopsFS where the HopsWorks project is located. To point to a particular dataset, this path should be\n",
    "    appended with the name of your dataset.\n",
    "\n",
    "    >>> from hops import hdfs\n",
    "    >>> project_path = hdfs.project_path()\n",
    "    >>> print(\"Project path: {}\".format(project_path))\n",
    "\n",
    "    Args:\n",
    "        :project: If this value is not specified, it will get the path to your project. If you need to path to another project, you can specify the name of the project as a string.\n",
    "\n",
    "    Returns:\n",
    "        returns the project absolute path\n",
    "    \"\"\"\n",
    "\n",
    "    if project is None:\n",
    "        project = project_name()\n",
    "\n",
    "    # abspath means \"hdfs://namenode:port/ is preprended\n",
    "    abspath = hdfs.path.abspath(\"/Projects/\" + project + \"/\")\n",
    "    if exclude_nn_addr:\n",
    "        abspath = re.sub(r\"\\d+.\\d+.\\d+.\\d+:\\d+\", \"\", abspath)\n",
    "    return abspath\n",
    "\n",
    "\n",
    "def get():\n",
    "    \"\"\" Get a handle to pydoop hdfs using the default namenode (specified in hadoop config)\n",
    "\n",
    "    Returns:\n",
    "        Pydoop hdfs handle\n",
    "    \"\"\"\n",
    "    return hdfs.hdfs('default', 0, user=project_user())\n",
    "\n",
    "\n",
    "def get_fs():\n",
    "    \"\"\" Get a handle to pydoop fs using the default namenode (specified in hadoop config)\n",
    "\n",
    "    Returns:\n",
    "        Pydoop fs handle\n",
    "    \"\"\"\n",
    "    return hdfs.fs.hdfs('default', 0, user=project_user())\n",
    "\n",
    "\n",
    "def _expand_path(hdfs_path, project=\"\", exists=True):\n",
    "    \"\"\"\n",
    "    Expands a given path. If the path is /Projects.. hdfs:// is prepended.\n",
    "    If the path is ../ the full project path is prepended.\n",
    "\n",
    "    Args:\n",
    "        :hdfs_path the path to be expanded\n",
    "        :exists boolean flag, if this is true an exception is thrown if the expanded path does not exist.\n",
    "\n",
    "    Raises:\n",
    "        IOError if exists flag is true and the path does not exist\n",
    "\n",
    "    Returns:\n",
    "        path expanded with HDFS and project\n",
    "    \"\"\"\n",
    "    if not isinstance(hdfs_path, string_types):\n",
    "        hdfs_path = hdfs_path.decode()\n",
    "    if project == \"\":\n",
    "        project = project_name()\n",
    "    # Check if a full path is supplied. If not, assume it is a relative path for this project - then build its full path and return it.\n",
    "    if hdfs_path.startswith(\"/Projects/\") or hdfs_path.startswith(\"/Projects\"):\n",
    "        hdfs_path = \"hdfs://\" + hdfs_path\n",
    "    elif not (hdfs_path.startswith(\"hdfs://\") or hdfs_path.startswith(\"hopsfs://\")):\n",
    "        # if the file URL type is not HDFS, throw an error\n",
    "        if \"://\" in hdfs_path:\n",
    "            raise IOError(\"path %s must be a full hopsfs path or a relative path\" % hdfs_path)\n",
    "        proj_path = project_path(project)\n",
    "        hdfs_path = proj_path + hdfs_path\n",
    "    if exists == True and not hdfs.path.exists(hdfs_path):\n",
    "        raise IOError(\"path %s not found\" % hdfs_path)\n",
    "    return hdfs_path\n",
    "\n",
    "def copy_to_hdfs(local_path, relative_hdfs_path, overwrite=False, project=None):\n",
    "    \"\"\"\n",
    "    Copies a path from local filesystem to HDFS project (recursively) using relative path in $CWD to a path in hdfs (hdfs_path)\n",
    "\n",
    "    For example, if you execute:\n",
    "\n",
    "    >>> copy_to_hdfs(\"data.tfrecords\", \"/Resources\", project=\"demo\")\n",
    "\n",
    "    This will copy the file data.tfrecords to hdfs://Projects/demo/Resources/data.tfrecords\n",
    "\n",
    "    Args:\n",
    "        :local_path: Absolute or local path on the local filesystem to copy\n",
    "        :relative_hdfs_path: a path in HDFS relative to the project root to where the local path should be written\n",
    "        :overwrite: a boolean flag whether to overwrite if the path already exists in HDFS\n",
    "        :project: name of the project, defaults to the current HDFS user's project\n",
    "    \"\"\"\n",
    "    if project == None:\n",
    "        project = project_name()\n",
    "\n",
    "    # Absolute path\n",
    "    if os.path.isabs(local_path):\n",
    "        full_local = local_path\n",
    "    else:\n",
    "        full_local = os.getcwd() + '/' + local_path\n",
    "\n",
    "    hdfs_path = _expand_path(relative_hdfs_path, project, exists=False)\n",
    "\n",
    "    if overwrite:\n",
    "        hdfs_path = hdfs_path + \"/\" + os.path.basename(full_local)\n",
    "        if exists(hdfs_path):\n",
    "            # delete hdfs path since overwrite flag was set to true\n",
    "            delete(hdfs_path, recursive=True)\n",
    "\n",
    "    log.debug(\"Started copying local path {} to hdfs path {}\\n\".format(local_path, hdfs_path))\n",
    "\n",
    "    # copy directories from local path to HDFS project path\n",
    "    hdfs.put(full_local, hdfs_path)\n",
    "\n",
    "    log.debug(\"Finished copying\\n\")\n",
    "\n",
    "\n",
    "def delete(hdfs_path, recursive=False):\n",
    "    \"\"\"\n",
    "    Deletes path, path can be absolute or relative.\n",
    "    If recursive is set to True and path is a directory, then files will be deleted recursively.\n",
    "\n",
    "    For example\n",
    "\n",
    "    >>> delete(\"/Resources/\", recursive=True)\n",
    "\n",
    "    will delete all files recursively in the folder \"Resources\" inside the current project.\n",
    "\n",
    "    Args:\n",
    "        :hdfs_path: the path to delete (project-relative or absolute)\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "\n",
    "    Raises:\n",
    "        IOError when recursive is False and directory is non-empty\n",
    "    \"\"\"\n",
    "    hdfs_path = _expand_path(hdfs_path)\n",
    "    hdfs_handle = get()\n",
    "    if hdfs_handle.exists(hdfs_path):\n",
    "        hdfs_handle.delete(hdfs_path, recursive=recursive)\n",
    "\n",
    "\n",
    "def copy_to_local(hdfs_path, local_path=\"\", overwrite=False, project=None):\n",
    "    \"\"\"\n",
    "    Copies a directory or file from a HDFS project to a local private scratch directory. If there is not enough space on the local scratch directory, an exception is thrown.\n",
    "    If the local file exists, and the hdfs file and the local file are the same size in bytes, return 'ok' immediately.\n",
    "    If the local directory tree exists, and the hdfs subdirectory and the local subdirectory have the same files and directories, return 'ok' immediately.\n",
    "\n",
    "    For example, if you execute:\n",
    "\n",
    "    >>> copy_to_local(\"Resources/my_data\")\n",
    "\n",
    "    This will copy the directory my_data from the Resources dataset in your project to the current working directory on the path ./my_data\n",
    "\n",
    "    Raises:\n",
    "      IOError if there is not enough space to localize the file/directory in HDFS to the scratch directory ($PDIR)\n",
    "\n",
    "    Args:\n",
    "        :hdfs_path: You can specify either a full hdfs pathname or a relative one (relative to your Project's path in HDFS).\n",
    "        :local_path: the relative or full path to a directory on the local filesystem to copy to (relative to a scratch directory $PDIR), defaults to $CWD\n",
    "        :overwrite: a boolean flag whether to overwrite if the path already exists in the local scratch directory.\n",
    "        :project: name of the project, defaults to the current HDFS user's project\n",
    "\n",
    "    Returns:\n",
    "        the full local pathname of the file/dir\n",
    "    \"\"\"\n",
    "\n",
    "    if project == None:\n",
    "        project = project_name()\n",
    "\n",
    "    if os.path.isabs(local_path):\n",
    "        local_dir = local_path\n",
    "    else:\n",
    "        local_dir = os.getcwd() + '/' + local_path\n",
    "\n",
    "    if not os.path.isdir(local_dir):\n",
    "        raise IOError(\"You need to supply the path to a local directory. This is not a local dir: %s\" % local_dir)\n",
    "\n",
    "    filename = path.basename(hdfs_path)\n",
    "    full_local = local_dir + \"/\" + filename\n",
    "\n",
    "    project_hdfs_path = _expand_path(hdfs_path, project=project)\n",
    "\n",
    "    # Get the amount of free space on the local drive\n",
    "    stat = os.statvfs(local_dir)\n",
    "    free_space_bytes = stat.f_bsize * stat.f_bavail\n",
    "\n",
    "    hdfs_size = path.getsize(project_hdfs_path)\n",
    "\n",
    "    if os.path.isfile(full_local) and not overwrite:\n",
    "        sz = os.path.getsize(full_local)\n",
    "        if hdfs_size == sz:\n",
    "            log.info(\"File \" + project_hdfs_path + \" is already localized, skipping download...\")\n",
    "            return full_local\n",
    "        else:\n",
    "            os.remove(full_local)\n",
    "\n",
    "    if os.path.isdir(full_local) and not overwrite:\n",
    "        try:\n",
    "            localized = _is_same_directory(full_local, project_hdfs_path)\n",
    "            if localized:\n",
    "                log.info(\"Full directory subtree already on local disk and unchanged. Set overwrite=True to force download\")\n",
    "                return full_local\n",
    "            else:\n",
    "                shutil.rmtree(full_local)\n",
    "        except Exception as e:\n",
    "            log.error(\"Failed while checking directory structure to avoid re-downloading dataset, falling back to downloading\")\n",
    "            log.error(e)\n",
    "            shutil.rmtree(full_local)\n",
    "\n",
    "    if hdfs_size > free_space_bytes:\n",
    "        raise IOError(\"Not enough local free space available on scratch directory: %s\" % local_path)\n",
    "\n",
    "    if overwrite:\n",
    "        if os.path.isdir(full_local):\n",
    "            shutil.rmtree(full_local)\n",
    "        elif os.path.isfile(full_local):\n",
    "            os.remove(full_local)\n",
    "\n",
    "    log.debug(\"Started copying \" + project_hdfs_path + \" to local disk on path \" + local_dir + \"\\n\")\n",
    "\n",
    "    hdfs.get(project_hdfs_path, local_dir)\n",
    "\n",
    "    log.debug(\"Finished copying\\n\")\n",
    "\n",
    "    return full_local\n",
    "\n",
    "\n",
    "def _is_same_directory(local_path, hdfs_path):\n",
    "    \"\"\"\n",
    "    Validates that the same occurrence and names of files exists in both hdfs and local\n",
    "    \"\"\"\n",
    "    local_file_list = []\n",
    "    for root, dirnames, filenames in os.walk(local_path):\n",
    "        for filename in fnmatch.filter(filenames, '*'):\n",
    "            local_file_list.append(filename)\n",
    "        for dirname in fnmatch.filter(dirnames, '*'):\n",
    "            local_file_list.append(dirname)\n",
    "    local_file_list.sort()\n",
    "\n",
    "    hdfs_file_list = glob(hdfs_path + '/*', recursive=True)\n",
    "    hdfs_file_list = [path.basename(str(r)) for r in hdfs_file_list]\n",
    "    hdfs_file_list.sort()\n",
    "\n",
    "    if local_file_list == hdfs_file_list:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "def cp(src_hdfs_path, dest_hdfs_path, overwrite=False):\n",
    "    \"\"\"\n",
    "    Copy the contents of src_hdfs_path to dest_hdfs_path.\n",
    "\n",
    "    If src_hdfs_path is a directory, its contents will be copied recursively.\n",
    "    Source file(s) are opened for reading and copies are opened for writing.\n",
    "\n",
    "    Args:\n",
    "        :src_hdfs_path: You can specify either a full hdfs pathname or a relative one (relative to your Project's path in HDFS).\n",
    "        :dest_hdfs_path: You can specify either a full hdfs pathname or a relative one (relative to your Project's path in HDFS).\n",
    "        :overwrite: boolean flag whether to overwrite destination path or not.\n",
    "\n",
    "    \"\"\"\n",
    "    src_hdfs_path = _expand_path(src_hdfs_path)\n",
    "    dest_hdfs_path = _expand_path(dest_hdfs_path, exists=False)\n",
    "\n",
    "    if overwrite and exists(dest_hdfs_path):\n",
    "        # delete path since overwrite flag was set to true\n",
    "        delete(dest_hdfs_path, recursive=True)\n",
    "\n",
    "    hdfs.cp(src_hdfs_path, dest_hdfs_path)\n",
    "\n",
    "def glob(hdfs_path, recursive=False, project=None):\n",
    "    \"\"\"\n",
    "    Finds all the pathnames matching a specified pattern according to the rules used by the Unix shell, although results are returned in arbitrary order.\n",
    "\n",
    "    Globbing gives you the list of files in a dir that matches a supplied pattern\n",
    "\n",
    "    >>> glob('Resources/*.json')\n",
    "    >>> ['Resources/1.json', 'Resources/2.json']\n",
    "\n",
    "    glob is implemented as  os.listdir() and fnmatch.fnmatch()\n",
    "    We implement glob as hdfs.ls() and fnmatch.filter()\n",
    "\n",
    "    Args:\n",
    "     :hdfs_path: You can specify either a full hdfs pathname or a relative one (relative to project_name in HDFS.\n",
    "     :project: If the supplied hdfs_path is a relative path, it will look for that file in this project's subdir in HDFS.\n",
    "\n",
    "    Raises:\n",
    "        IOError if the supplied hdfs path does not exist\n",
    "\n",
    "    Returns:\n",
    "      A possibly-empty list of path names that match pathname, which must be a string containing a path specification. pathname can be either absolute\n",
    "    \"\"\"\n",
    "\n",
    "    # Get the full path to the dir for the input glob pattern\n",
    "    # \"hdfs://Projects/jim/blah/*.jpg\" => \"hdfs://Projects/jim/blah\"\n",
    "    # Then, ls on 'hdfs://Projects/jim/blah', then filter out results\n",
    "    if project == None:\n",
    "        project = project_name()\n",
    "    lastSep = hdfs_path.rfind(\"/\")\n",
    "    inputDir = hdfs_path[:lastSep]\n",
    "    inputDir = _expand_path(inputDir, project)\n",
    "    pattern = hdfs_path[lastSep + 1:]\n",
    "    if not hdfs.path.exists(inputDir):\n",
    "        raise IOError(\"Glob path %s not found\" % inputDir)\n",
    "    dirContents = hdfs.ls(inputDir, recursive=recursive)\n",
    "    return fnmatch.filter(dirContents, pattern)\n",
    "\n",
    "\n",
    "def ls(hdfs_path, recursive=False, project=None):\n",
    "    \"\"\"\n",
    "    Returns all the pathnames in the supplied directory.\n",
    "\n",
    "    Args:\n",
    "        :hdfs_path: You can specify either a full hdfs pathname or a relative one (relative to project_name in HDFS).\n",
    "        :recursive: if it is a directory and recursive is True, the list contains one item for every file or directory in the tree rooted at hdfs_path.\n",
    "        :project: If the supplied hdfs_path is a relative path, it will look for that file in this project's subdir in HDFS.\n",
    "\n",
    "    Returns:\n",
    "      A possibly-empty list of path names stored in the supplied path.\n",
    "    \"\"\"\n",
    "    if project == None:\n",
    "        project = project_name()\n",
    "    hdfs_path = _expand_path(hdfs_path, project)\n",
    "    return hdfs.ls(hdfs_path, recursive=recursive)\n",
    "\n",
    "\n",
    "def lsl(hdfs_path, recursive=False, project=None):\n",
    "    \"\"\"\n",
    "    Returns all the pathnames in the supplied directory.\n",
    "\n",
    "    Args:\n",
    "        :hdfs_path: You can specify either a full hdfs pathname or a relative one (relative to project_name in HDFS).\n",
    "        :recursive: if it is a directory and recursive is True, the list contains one item for every file or directory in the tree rooted at hdfs_path.\n",
    "        :project: If the supplied hdfs_path is a relative path, it will look for that file in this project's subdir in HDFS.\n",
    "\n",
    "    Returns:\n",
    "        A possibly-empty list of path names stored in the supplied path.\n",
    "    \"\"\"\n",
    "    if project == None:\n",
    "        project = project_name()\n",
    "    hdfs_path = _expand_path(hdfs_path, project)\n",
    "    return hdfs.lsl(hdfs_path, recursive=recursive)\n",
    "\n",
    "\n",
    "def rmr(hdfs_path, project=None):\n",
    "    \"\"\"\n",
    "    Recursively remove files and directories.\n",
    "\n",
    "    Args:\n",
    "        :hdfs_path: You can specify either a full hdfs pathname or a relative one (relative to project_name in HDFS).\n",
    "        :project: If the supplied hdfs_path is a relative path, it will look for that file in this project's subdir in HDFS.\n",
    "\n",
    "    \"\"\"\n",
    "    if project == None:\n",
    "        project = project_name()\n",
    "    hdfs_path = _expand_path(hdfs_path, project)\n",
    "    return hdfs.rmr(hdfs_path)\n",
    "\n",
    "\n",
    "def mkdir(hdfs_path, project=None):\n",
    "    \"\"\"\n",
    "    Create a directory and its parents as needed.\n",
    "\n",
    "    Args:\n",
    "        :hdfs_path: You can specify either a full hdfs pathname or a relative one (relative to project_name in HDFS).\n",
    "        :project: If the supplied hdfs_path is a relative path, it will look for that file in this project's subdir in HDFS.\n",
    "\n",
    "    \"\"\"\n",
    "    if project == None:\n",
    "        project = project_name()\n",
    "    hdfs_path = _expand_path(hdfs_path, project, exists=False)\n",
    "    return hdfs.mkdir(hdfs_path)\n",
    "\n",
    "\n",
    "def move(src, dest):\n",
    "    \"\"\"\n",
    "    Move or rename src to dest.\n",
    "\n",
    "    Args:\n",
    "        :src: You can specify either a full hdfs pathname or a relative one (relative to your Project's path in HDFS).\n",
    "        :dest: You can specify either a full hdfs pathname or a relative one (relative to your Project's path in HDFS).\n",
    "\n",
    "    \"\"\"\n",
    "    src = _expand_path(src, project_name())\n",
    "    dest = _expand_path(dest, project_name(), exists=False)\n",
    "    return hdfs.move(src, dest)\n",
    "\n",
    "\n",
    "def rename(src, dest):\n",
    "    \"\"\"\n",
    "    Rename src to dest.\n",
    "\n",
    "    Args:\n",
    "        :src: You can specify either a full hdfs pathname or a relative one (relative to your Project's path in HDFS).\n",
    "        :dest: You can specify either a full hdfs pathname or a relative one (relative to your Project's path in HDFS).\n",
    "    \"\"\"\n",
    "    src = _expand_path(src, project_name())\n",
    "    dest = _expand_path(dest, project_name(), exists=False)\n",
    "    return hdfs.rename(src, dest)\n",
    "\n",
    "\n",
    "def chown(hdfs_path, user, group, project=None):\n",
    "    \"\"\"\n",
    "    Change file owner and group.\n",
    "\n",
    "    Args:\n",
    "        :hdfs_path: You can specify either a full hdfs pathname or a relative one (relative to the given project path in HDFS).\n",
    "        :user: New hdfs username\n",
    "        :group: New hdfs group\n",
    "        :project: If this value is not specified, it will get the path to your project. If you need to path to another project, you can specify the name of the project as a string.\n",
    "    \"\"\"\n",
    "    if project == None:\n",
    "        project = project_name()\n",
    "    hdfs_path = _expand_path(hdfs_path, project)\n",
    "    return hdfs.chown(hdfs_path, user, group)\n",
    "\n",
    "\n",
    "def chmod(hdfs_path, mode, project=None):\n",
    "    \"\"\"\n",
    "    Change file mode bits.\n",
    "\n",
    "    Args:\n",
    "        :hdfs_path: You can specify either a full hdfs pathname or a relative one (relative to your Project's path in HDFS).\n",
    "        :mode: File mode (user/group/world privilege) bits\n",
    "        :project: If this value is not specified, it will get the path to your project. If you need to path to another project, you can specify the name of the project as a string.\n",
    "    \"\"\"\n",
    "    if project == None:\n",
    "        project = project_name()\n",
    "    hdfs_path = _expand_path(hdfs_path, project)\n",
    "    return hdfs.chmod(hdfs_path, mode)\n",
    "\n",
    "\n",
    "def stat(hdfs_path, project=None):\n",
    "    \"\"\"\n",
    "    Performs the equivalent of os.stat() on path, returning a StatResult object.\n",
    "\n",
    "    Args:\n",
    "        :hdfs_path: If this value is not specified, it will get the path to your project. You can specify either a full hdfs pathname or a relative one (relative to your Project's path in HDFS).\n",
    "        :project: If this value is not specified, it will get the path to your project. If you need to path to another project, you can specify the name of the project as a string.\n",
    "\n",
    "    Returns:\n",
    "        StatResult object\n",
    "    \"\"\"\n",
    "    if project == None:\n",
    "        project = project_name()\n",
    "    hdfs_path = _expand_path(hdfs_path, project)\n",
    "    return hdfs.stat(hdfs_path)\n",
    "\n",
    "\n",
    "def access(hdfs_path, mode, project=None):\n",
    "    \"\"\"\n",
    "    Perform the equivalent of os.access() on path.\n",
    "\n",
    "    Args:\n",
    "        :hdfs_path: You can specify either a full hdfs pathname or a relative one (relative to your Project's path in HDFS).\n",
    "        :mode: File mode (user/group/world privilege) bits\n",
    "        :project: If this value is not specified, it will get the path to your project. If you need to path to another project, you can specify the name of the project as a string.\n",
    "\n",
    "    Returns:\n",
    "        True if access is allowed, False if not.\n",
    "    \"\"\"\n",
    "    if project == None:\n",
    "        project = project_name()\n",
    "    hdfs_path = _expand_path(hdfs_path, project)\n",
    "    return hdfs.access(hdfs_path, mode)\n",
    "\n",
    "\n",
    "def _mkdir_p(path):\n",
    "    \"\"\"\n",
    "    Creates path on local filesystem\n",
    "\n",
    "    Args:\n",
    "        path to create\n",
    "\n",
    "    Raises:\n",
    "        OSError\n",
    "    \"\"\"\n",
    "    try:\n",
    "        os.makedirs(path)\n",
    "    except OSError as exc:\n",
    "        if exc.errno == errno.EEXIST and os.path.isdir(path):\n",
    "            pass\n",
    "        else:\n",
    "            raise\n",
    "\n",
    "\n",
    "def open_file(hdfs_path, project=None, flags='rw', buff_size=0):\n",
    "    \"\"\"\n",
    "    Opens an HDFS file for read/write/append and returns a file descriptor object (fd) that should be closed when no longer needed.\n",
    "\n",
    "    Args:\n",
    "        hdfs_path: you can specify either a full hdfs pathname or a relative one (relative to your project's path in HDFS)\n",
    "        flags: supported opening modes are 'r', 'w', 'a'. In addition, a trailing 't' can be added to specify text mode (e.g, 'rt' = open for reading text)\n",
    "        buff_size: Pass 0 as buff_size if you want to use the \"configured\" values, i.e the ones set in the Hadoop configuration files.\n",
    "\n",
    "    Returns:\n",
    "        A file descriptor (fd) that needs to be closed (fd-close()) when it is no longer needed.\n",
    "\n",
    "    Raises:\n",
    "        IOError: If the file does not exist.\n",
    "    \"\"\"\n",
    "    if project == None:\n",
    "        project = project_name()\n",
    "    hdfs_path = _expand_path(hdfs_path, project, exists=False)\n",
    "    fs_handle = get_fs()\n",
    "    fd = fs_handle.open_file(hdfs_path, flags, buff_size=buff_size)\n",
    "    return fd\n",
    "\n",
    "\n",
    "def close():\n",
    "    \"\"\"\n",
    "    Closes an the HDFS connection (disconnects to the namenode)\n",
    "    \"\"\"\n",
    "    hdfs.close()\n",
    "\n",
    "\n",
    "def exists(hdfs_path, project=None):\n",
    "    \"\"\"\n",
    "    Return True if hdfs_path exists in the default HDFS.\n",
    "\n",
    "    Args:\n",
    "        :hdfs_path: You can specify either a full hdfs pathname or a relative one (relative to your Project's path in HDFS).\n",
    "        :project: If this value is not specified, it will get the path to your project. If you need to path to another project, you can specify the name of the project as a string.\n",
    "\n",
    "\n",
    "    Returns:\n",
    "        True if hdfs_path exists.\n",
    "\n",
    "    Raises: IOError\n",
    "    \"\"\"\n",
    "    if project == None:\n",
    "        project = project_name()\n",
    "\n",
    "    try:\n",
    "        hdfs_path = _expand_path(hdfs_path, project)\n",
    "    except IOError:\n",
    "        return False\n",
    "    return hdfs.path.exists(hdfs_path)\n",
    "\n",
    "\n",
    "def isdir(hdfs_path, project=None):\n",
    "    \"\"\"\n",
    "    Return True if path refers to a directory.\n",
    "\n",
    "    Args:\n",
    "        :hdfs_path: You can specify either a full hdfs pathname or a relative one (relative to your Project's path in HDFS).\n",
    "        :project: If this value is not specified, it will get the path to your project. If you need to path to another project, you can specify the name of the project as a string.\n",
    "\n",
    "    Returns:\n",
    "        True if path refers to a directory.\n",
    "\n",
    "    Raises: IOError\n",
    "    \"\"\"\n",
    "    if project == None:\n",
    "        project = project_name()\n",
    "    hdfs_path = _expand_path(hdfs_path, project)\n",
    "    return hdfs.isdir(hdfs_path)\n",
    "\n",
    "\n",
    "def isfile(hdfs_path, project=None):\n",
    "    \"\"\"\n",
    "    Return True if path refers to a file.\n",
    "\n",
    "    Args:\n",
    "        :hdfs_path: You can specify either a full hdfs pathname or a relative one (relative to your Project's path in HDFS).\n",
    "        :project: If this value is not specified, it will get the path to your project. If you need to path to another project, you can specify the name of the project as a string.\n",
    "\n",
    "    Returns:\n",
    "        True if path refers to a file.\n",
    "\n",
    "    Raises: IOError\n",
    "    \"\"\"\n",
    "    if project == None:\n",
    "        project = project_name()\n",
    "    hdfs_path = _expand_path(hdfs_path, project)\n",
    "    return path.isfile(hdfs_path)\n",
    "\n",
    "def isdir(hdfs_path, project=None):\n",
    "    \"\"\"\n",
    "    Return True if path refers to a directory.\n",
    "\n",
    "    Args:\n",
    "        :hdfs_path: You can specify either a full hdfs pathname or a relative one (relative to your Project's path in HDFS).\n",
    "        :project: If this value is not specified, it will get the path to your project. If you need to path to another project, you can specify the name of the project as a string.\n",
    "\n",
    "    Returns:\n",
    "        True if path refers to a file.\n",
    "\n",
    "    Raises: IOError\n",
    "    \"\"\"\n",
    "    if project == None:\n",
    "        project = project_name()\n",
    "    hdfs_path = _expand_path(hdfs_path, project)\n",
    "    return path.isdir(hdfs_path)\n",
    "\n",
    "\n",
    "def capacity():\n",
    "    \"\"\"\n",
    "    Returns the raw capacity of the filesystem\n",
    "\n",
    "    Returns:\n",
    "        filesystem capacity (int)\n",
    "    \"\"\"\n",
    "    return hdfs.capacity()\n",
    "\n",
    "\n",
    "def dump(data, hdfs_path):\n",
    "    \"\"\"\n",
    "    Dumps data to a file\n",
    "\n",
    "    Args:\n",
    "        :data: data to write to hdfs_path\n",
    "        :hdfs_path: You can specify either a full hdfs pathname or a relative one (relative to your Project's path in HDFS).\n",
    "    \"\"\"\n",
    "\n",
    "    hdfs_path = _expand_path(hdfs_path, exists=False)\n",
    "    return hdfs.dump(data, hdfs_path)\n",
    "\n",
    "\n",
    "def load(hdfs_path):\n",
    "    \"\"\"\n",
    "    Read the content of hdfs_path and return it.\n",
    "\n",
    "    Args:\n",
    "        :hdfs_path: You can specify either a full hdfs pathname or a relative one (relative to your Project's path in HDFS).\n",
    "\n",
    "    Returns:\n",
    "        the read contents of hdfs_path\n",
    "    \"\"\"\n",
    "    hdfs_path = _expand_path(hdfs_path)\n",
    "    return hdfs.load(hdfs_path)\n",
    "\n",
    "def ls(hdfs_path, recursive=False, exclude_nn_addr=False):\n",
    "    \"\"\"\n",
    "    lists a directory in HDFS\n",
    "\n",
    "    Args:\n",
    "        :hdfs_path: You can specify either a full hdfs pathname or a relative one (relative to your Project's path in HDFS).\n",
    "\n",
    "    Returns:\n",
    "        returns a list of hdfs paths\n",
    "    \"\"\"\n",
    "    if exclude_nn_addr:\n",
    "        hdfs_path = re.sub(r\"\\d+.\\d+.\\d+.\\d+:\\d+\", \"\", hdfs_path)\n",
    "    hdfs_path = _expand_path(hdfs_path)\n",
    "    return hdfs.ls(hdfs_path, recursive=recursive)\n",
    "\n",
    "def stat(hdfs_path):\n",
    "    \"\"\"\n",
    "    Performs the equivalent of os.stat() on hdfs_path, returning a StatResult object.\n",
    "\n",
    "    Args:\n",
    "        :hdfs_path: You can specify either a full hdfs pathname or a relative one (relative to your Project's path in HDFS).\n",
    "\n",
    "    Returns:\n",
    "        returns a list of hdfs paths\n",
    "    \"\"\"\n",
    "    hdfs_path = _expand_path(hdfs_path)\n",
    "    return hdfs.stat(hdfs_path)\n",
    "\n",
    "def abs_path(hdfs_path):\n",
    "    \"\"\"\n",
    "     Return an absolute path for hdfs_path.\n",
    "\n",
    "     Args:\n",
    "         :hdfs_path: You can specify either a full hdfs pathname or a relative one (relative to your Project's path in HDFS).\n",
    "\n",
    "    Returns:\n",
    "        Return an absolute path for hdfs_path.\n",
    "    \"\"\"\n",
    "    return _expand_path(hdfs_path)\n",
    "\n",
    "def add_module(hdfs_path, project=None):\n",
    "    \"\"\"\n",
    "     Add a .py or .ipynb file from HDFS to sys.path\n",
    "\n",
    "     For example, if you execute:\n",
    "\n",
    "     >>> add_module(\"Resources/my_module.py\")\n",
    "     >>> add_module(\"Resources/my_notebook.ipynb\")\n",
    "\n",
    "     You can import it simply as:\n",
    "\n",
    "     >>> import my_module\n",
    "     >>> import my_notebook\n",
    "\n",
    "     Args:\n",
    "         :hdfs_path: You can specify either a full hdfs pathname or a relative one (relative to your Project's path in HDFS) to a .py or .ipynb file\n",
    "\n",
    "     Returns:\n",
    "        Return full local path to localized python file or converted python file in case of .ipynb file\n",
    "    \"\"\"\n",
    "\n",
    "    localized_deps = os.getcwd() + \"/localized_deps\"\n",
    "    if not os.path.exists(localized_deps):\n",
    "        os.mkdir(localized_deps)\n",
    "        open(localized_deps + '/__init__.py', mode='w').close()\n",
    "\n",
    "    if localized_deps not in sys.path:\n",
    "        sys.path.append(localized_deps)\n",
    "\n",
    "    if project == None:\n",
    "        project = project_name()\n",
    "    hdfs_path = _expand_path(hdfs_path, project)\n",
    "\n",
    "    if path.isfile(hdfs_path) and hdfs_path.endswith('.py'):\n",
    "        py_path = copy_to_local(hdfs_path, localized_deps, overwrite=True)\n",
    "        if py_path not in sys.path:\n",
    "            sys.path.append(py_path)\n",
    "        _reload(py_path)\n",
    "        return py_path\n",
    "    elif path.isfile(hdfs_path) and hdfs_path.endswith('.ipynb'):\n",
    "        ipynb_path = copy_to_local(hdfs_path, localized_deps, overwrite=True)\n",
    "        python_path = sys.executable\n",
    "        jupyter_binary = os.path.dirname(python_path) + '/jupyter'\n",
    "        if not os.path.exists(jupyter_binary):\n",
    "            raise Exception('Could not find jupyter binary on path {}'.format(jupyter_binary))\n",
    "\n",
    "        converted_py_path = os.path.splitext(ipynb_path)[0] + '.py'\n",
    "        if os.path.exists(converted_py_path):\n",
    "            os.remove(converted_py_path)\n",
    "\n",
    "        conversion = subprocess.Popen([jupyter_binary, 'nbconvert', '--to', 'python', ipynb_path], stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "        out, err = conversion.communicate()\n",
    "        if conversion.returncode != 0:\n",
    "            raise Exception(\"Notebook conversion to .py failed: stdout: {} \\n stderr: {}\".format(out, err))\n",
    "\n",
    "        if not os.path.exists(converted_py_path):\n",
    "            raise Exception('Could not find converted .py file on path {}'.format(converted_py_path))\n",
    "        if converted_py_path not in sys.path:\n",
    "            sys.path.append(converted_py_path)\n",
    "        _reload(converted_py_path)\n",
    "        return converted_py_path\n",
    "    else:\n",
    "        raise Exception(\"Given path \" + hdfs_path + \" does not point to a .py or .ipynb file\")\n",
    "\n",
    "def _reload(path):\n",
    "    try:\n",
    "        module_name = ntpath.basename(path).split(\".\")[0]\n",
    "        imported_module = importlib.import_module(module_name)\n",
    "        importlib.reload(imported_module)\n",
    "    except Exception as err:\n",
    "        log.error('Failed to automatically reload module on path {} with exception: {}'.format(path, err))\n",
    "\n",
    "def is_tls_enabled():\n",
    "    \"\"\"\n",
    "    Reads the ipc.server.ssl.enabled property from core-site.xml.\n",
    "\n",
    "    Returns:\n",
    "        returns True if ipc.server.ssl.enabled is true. False otherwise.\n",
    "    \"\"\"\n",
    "    global tls_enabled\n",
    "    if tls_enabled is None:\n",
    "        hadoop_conf_path = os.environ['HADOOP_CONF_DIR']\n",
    "        xmldoc = minidom.parse(os.path.join(hadoop_conf_path,'core-site.xml'))\n",
    "        itemlist = xmldoc.getElementsByTagName('property')\n",
    "        for item in itemlist:\n",
    "            name = item.getElementsByTagName(\"name\")[0]\n",
    "            if name.firstChild.data == \"ipc.server.ssl.enabled\":\n",
    "                tls_enabled = item.getElementsByTagName(\"value\")[0].firstChild.data == 'true'\n",
    "    return tls_enabled\n",
    "\n",
    "def _get_webhdfs_address():\n",
    "    \"\"\"\n",
    "    Makes an SRV DNS query to get the target and port of NameNode's web interface\n",
    "\n",
    "    Returns:\n",
    "        returns webhdfs endpoint\n",
    "    \"\"\"\n",
    "    global webhdfs_address\n",
    "    if webhdfs_address is None:\n",
    "        _, port = ServiceDiscovery.get_any_service('http.namenode')\n",
    "        webhdfs_address = ServiceDiscovery.construct_service_fqdn('http.namenode') + \":\" + str(port)\n",
    "    return webhdfs_address\n",
    "        \n",
    "def get_webhdfs_host():\n",
    "    \"\"\"\n",
    "    Makes an SRV DNS query and gets the actual hostname of the NameNode\n",
    "\n",
    "    Returns:\n",
    "        returns NameNode's hostname\n",
    "    \"\"\"\n",
    "    return ServiceDiscovery.construct_service_fqdn('http.namenode')\n",
    "\n",
    "def get_webhdfs_port():\n",
    "    \"\"\"\n",
    "    Makes an SRV DNS query and gets NameNode's port for WebHDFS\n",
    "\n",
    "    Returns:\n",
    "        returns NameNode's port for WebHDFS\n",
    "    \"\"\"\n",
    "    return _get_webhdfs_address().split(\":\")[1]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "612e34d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "Question 6. Implement a Python program that analyzes the storage utilization of DataNodes in a Hadoop cluster and identifies the nodes with the highest and lowest storage capacities.\n",
    "\n",
    "Solution:-  mapper.py\n",
    "    \n",
    "    #!/usr/bin/python\n",
    "  \n",
    "# import sys because we need to read and write data to STDIN and STDOUT\n",
    "import sys\n",
    "  \n",
    "# reading entire line from STDIN (standard input)\n",
    "for line in sys.stdin:\n",
    "    # to remove leading and trailing whitespace\n",
    "    line = line.strip()\n",
    "    # split the line into words\n",
    "    words = line.split()\n",
    "      \n",
    "    # we are looping over the words array and printing the word\n",
    "    # with the count of 1 to the STDOUT\n",
    "    for word in words:\n",
    "        # write the results to STDOUT (standard output);\n",
    "        # what we output here will be the input for the\n",
    "        # Reduce step, i.e. the input for reducer.py\n",
    "        print('%s\\t%s' % (word, 1))\n",
    "        \n",
    "        \n",
    "reducer.py\n",
    "\n",
    "#!/usr/bin/python\n",
    "  \n",
    "from operator import itemgetter\n",
    "import sys\n",
    "  \n",
    "current_word = None\n",
    "current_count = 0\n",
    "word = None\n",
    "  \n",
    "# read the entire line from STDIN\n",
    "for line in sys.stdin:\n",
    "    # remove leading and trailing whitespace\n",
    "    line = line.strip()\n",
    "    # slpiting the data on the basis of tab we have provided in mapper.py\n",
    "    word, count = line.split('\\t', 1)\n",
    "    # convert count (currently a string) to int\n",
    "    try:\n",
    "        count = int(count)\n",
    "    except ValueError:\n",
    "        # count was not a number, so silently\n",
    "        # ignore/discard this line\n",
    "        continue\n",
    "  \n",
    "    # this IF-switch only works because Hadoop sorts map output\n",
    "    # by key (here: word) before it is passed to the reducer\n",
    "    if current_word == word:\n",
    "        current_count += count\n",
    "    else:\n",
    "        if current_word:\n",
    "            # write result to STDOUT\n",
    "            print('%s\\t%s' % (current_word, current_count))\n",
    "        current_count = count\n",
    "        current_word = word\n",
    "  \n",
    "# do not forget to output the last word if needed!\n",
    "if current_word == word:\n",
    "    print('%s\\t%s' % (current_word, current_count))\n",
    "    \n",
    "\n",
    "demo.txt\n",
    "\n",
    "hello hi bye ok\n",
    "ok bye nice hello\n",
    "tata bye bye\n",
    "\n",
    "hadoop-streaming-2.4.0.jar->needed\n",
    "\n",
    "hdfs_commands.txt\n",
    "\n",
    "# In hdfs file system / is the root\n",
    "\n",
    "# Command to check the files inside root hdfs directory\n",
    "hadoop fs -ls /\n",
    "\n",
    "# Command to create directory in hdfs\n",
    "hadoop fs -mkdir /input_data\n",
    "\n",
    "\n",
    "# Copy data from local file system to Hdfs\n",
    "hadoop fs -put test_demo/trees.csv /input_data\n",
    "\n",
    "# Copy from HDFS path to local file system\n",
    "hadoop fs -copyToLocal /input_data/trees.csv ./\n",
    "\n",
    "# Command to execute map reduce code\n",
    "hadoop jar hadoop-streaming-2.4.0.jar -files mapper.py,reducer.py -mapper mapper.py -reducer reducer.py -input /test/demo.txt -output /output\n",
    "\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e22f81a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "Question 7. Create a Python script that interacts with YARN's ResourceManager API to submit a Hadoop job, monitor its progress, and retrieve the final output.\n",
    "\n",
    "Solution:-  mapper.py\n",
    "    \n",
    "    #!/usr/bin/python\n",
    "  \n",
    "# import sys because we need to read and write data to STDIN and STDOUT\n",
    "import sys\n",
    "  \n",
    "# reading entire line from STDIN (standard input)\n",
    "for line in sys.stdin:\n",
    "    # to remove leading and trailing whitespace\n",
    "    line = line.strip()\n",
    "    # split the line into words\n",
    "    words = line.split()\n",
    "      \n",
    "    # we are looping over the words array and printing the word\n",
    "    # with the count of 1 to the STDOUT\n",
    "    for word in words:\n",
    "        # write the results to STDOUT (standard output);\n",
    "        # what we output here will be the input for the\n",
    "        # Reduce step, i.e. the input for reducer.py\n",
    "        print('%s\\t%s' % (word, 1))\n",
    "        \n",
    "        \n",
    "reducer.py\n",
    "\n",
    "#!/usr/bin/python\n",
    "  \n",
    "from operator import itemgetter\n",
    "import sys\n",
    "  \n",
    "current_word = None\n",
    "current_count = 0\n",
    "word = None\n",
    "  \n",
    "# read the entire line from STDIN\n",
    "for line in sys.stdin:\n",
    "    # remove leading and trailing whitespace\n",
    "    line = line.strip()\n",
    "    # slpiting the data on the basis of tab we have provided in mapper.py\n",
    "    word, count = line.split('\\t', 1)\n",
    "    # convert count (currently a string) to int\n",
    "    try:\n",
    "        count = int(count)\n",
    "    except ValueError:\n",
    "        # count was not a number, so silently\n",
    "        # ignore/discard this line\n",
    "        continue\n",
    "  \n",
    "    # this IF-switch only works because Hadoop sorts map output\n",
    "    # by key (here: word) before it is passed to the reducer\n",
    "    if current_word == word:\n",
    "        current_count += count\n",
    "    else:\n",
    "        if current_word:\n",
    "            # write result to STDOUT\n",
    "            print('%s\\t%s' % (current_word, current_count))\n",
    "        current_count = count\n",
    "        current_word = word\n",
    "  \n",
    "# do not forget to output the last word if needed!\n",
    "if current_word == word:\n",
    "    print('%s\\t%s' % (current_word, current_count))\n",
    "    \n",
    "\n",
    "demo.txt\n",
    "\n",
    "hello hi bye ok\n",
    "ok bye nice hello\n",
    "tata bye bye\n",
    "\n",
    "hadoop-streaming-2.4.0.jar->needed\n",
    "\n",
    "hdfs_commands.txt\n",
    "\n",
    "# In hdfs file system / is the root\n",
    "\n",
    "# Command to check the files inside root hdfs directory\n",
    "hadoop fs -ls /\n",
    "\n",
    "# Command to create directory in hdfs\n",
    "hadoop fs -mkdir /input_data\n",
    "\n",
    "\n",
    "# Copy data from local file system to Hdfs\n",
    "hadoop fs -put test_demo/trees.csv /input_data\n",
    "\n",
    "# Copy from HDFS path to local file system\n",
    "hadoop fs -copyToLocal /input_data/trees.csv ./\n",
    "\n",
    "# Command to execute map reduce code\n",
    "hadoop jar hadoop-streaming-2.4.0.jar -files mapper.py,reducer.py -mapper mapper.py -reducer reducer.py -input /test/demo.txt -output /output\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c458ced",
   "metadata": {},
   "outputs": [],
   "source": [
    "Question 8. Create a Python script that interacts with YARN's ResourceManager API to submit a Hadoop job, set resource requirements, and track resource usage during job execution.\n",
    "\n",
    "Solution:-  mapper.py\n",
    "    \n",
    "    #!/usr/bin/python\n",
    "  \n",
    "# import sys because we need to read and write data to STDIN and STDOUT\n",
    "import sys\n",
    "  \n",
    "# reading entire line from STDIN (standard input)\n",
    "for line in sys.stdin:\n",
    "    # to remove leading and trailing whitespace\n",
    "    line = line.strip()\n",
    "    # split the line into words\n",
    "    words = line.split()\n",
    "      \n",
    "    # we are looping over the words array and printing the word\n",
    "    # with the count of 1 to the STDOUT\n",
    "    for word in words:\n",
    "        # write the results to STDOUT (standard output);\n",
    "        # what we output here will be the input for the\n",
    "        # Reduce step, i.e. the input for reducer.py\n",
    "        print('%s\\t%s' % (word, 1))\n",
    "        \n",
    "        \n",
    "reducer.py\n",
    "\n",
    "#!/usr/bin/python\n",
    "  \n",
    "from operator import itemgetter\n",
    "import sys\n",
    "  \n",
    "current_word = None\n",
    "current_count = 0\n",
    "word = None\n",
    "  \n",
    "# read the entire line from STDIN\n",
    "for line in sys.stdin:\n",
    "    # remove leading and trailing whitespace\n",
    "    line = line.strip()\n",
    "    # slpiting the data on the basis of tab we have provided in mapper.py\n",
    "    word, count = line.split('\\t', 1)\n",
    "    # convert count (currently a string) to int\n",
    "    try:\n",
    "        count = int(count)\n",
    "    except ValueError:\n",
    "        # count was not a number, so silently\n",
    "        # ignore/discard this line\n",
    "        continue\n",
    "  \n",
    "    # this IF-switch only works because Hadoop sorts map output\n",
    "    # by key (here: word) before it is passed to the reducer\n",
    "    if current_word == word:\n",
    "        current_count += count\n",
    "    else:\n",
    "        if current_word:\n",
    "            # write result to STDOUT\n",
    "            print('%s\\t%s' % (current_word, current_count))\n",
    "        current_count = count\n",
    "        current_word = word\n",
    "  \n",
    "# do not forget to output the last word if needed!\n",
    "if current_word == word:\n",
    "    print('%s\\t%s' % (current_word, current_count))\n",
    "    \n",
    "\n",
    "demo.txt\n",
    "\n",
    "hello hi bye ok\n",
    "ok bye nice hello\n",
    "tata bye bye\n",
    "\n",
    "hadoop-streaming-2.4.0.jar->needed\n",
    "\n",
    "hdfs_commands.txt\n",
    "\n",
    "# In hdfs file system / is the root\n",
    "\n",
    "# Command to check the files inside root hdfs directory\n",
    "hadoop fs -ls /\n",
    "\n",
    "# Command to create directory in hdfs\n",
    "hadoop fs -mkdir /input_data\n",
    "\n",
    "\n",
    "# Copy data from local file system to Hdfs\n",
    "hadoop fs -put test_demo/trees.csv /input_data\n",
    "\n",
    "# Copy from HDFS path to local file system\n",
    "hadoop fs -copyToLocal /input_data/trees.csv ./\n",
    "\n",
    "# Command to execute map reduce code\n",
    "hadoop jar hadoop-streaming-2.4.0.jar -files mapper.py,reducer.py -mapper mapper.py -reducer reducer.py -input /test/demo.txt -output /output\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8b856bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "Question 9. Write a Python program that compares the performance of a MapReduce job with different input split sizes, showcasing the impact on overall job execution time.\n",
    "\n",
    "Solution:- mapper.py\n",
    "    \n",
    "    #!/usr/bin/python\n",
    "  \n",
    "# import sys because we need to read and write data to STDIN and STDOUT\n",
    "import sys\n",
    "  \n",
    "# reading entire line from STDIN (standard input)\n",
    "for line in sys.stdin:\n",
    "    # to remove leading and trailing whitespace\n",
    "    line = line.strip()\n",
    "    # split the line into words\n",
    "    words = line.split()\n",
    "      \n",
    "    # we are looping over the words array and printing the word\n",
    "    # with the count of 1 to the STDOUT\n",
    "    for word in words:\n",
    "        # write the results to STDOUT (standard output);\n",
    "        # what we output here will be the input for the\n",
    "        # Reduce step, i.e. the input for reducer.py\n",
    "        print('%s\\t%s' % (word, 1))\n",
    "        \n",
    "        \n",
    "reducer.py\n",
    "\n",
    "#!/usr/bin/python\n",
    "  \n",
    "from operator import itemgetter\n",
    "import sys\n",
    "  \n",
    "current_word = None\n",
    "current_count = 0\n",
    "word = None\n",
    "  \n",
    "# read the entire line from STDIN\n",
    "for line in sys.stdin:\n",
    "    # remove leading and trailing whitespace\n",
    "    line = line.strip()\n",
    "    # slpiting the data on the basis of tab we have provided in mapper.py\n",
    "    word, count = line.split('\\t', 1)\n",
    "    # convert count (currently a string) to int\n",
    "    try:\n",
    "        count = int(count)\n",
    "    except ValueError:\n",
    "        # count was not a number, so silently\n",
    "        # ignore/discard this line\n",
    "        continue\n",
    "  \n",
    "    # this IF-switch only works because Hadoop sorts map output\n",
    "    # by key (here: word) before it is passed to the reducer\n",
    "    if current_word == word:\n",
    "        current_count += count\n",
    "    else:\n",
    "        if current_word:\n",
    "            # write result to STDOUT\n",
    "            print('%s\\t%s' % (current_word, current_count))\n",
    "        current_count = count\n",
    "        current_word = word\n",
    "  \n",
    "# do not forget to output the last word if needed!\n",
    "if current_word == word:\n",
    "    print('%s\\t%s' % (current_word, current_count))\n",
    "    \n",
    "\n",
    "demo.txt\n",
    "\n",
    "hello hi bye ok\n",
    "ok bye nice hello\n",
    "tata bye bye\n",
    "\n",
    "hadoop-streaming-2.4.0.jar->needed\n",
    "\n",
    "hdfs_commands.txt\n",
    "\n",
    "# In hdfs file system / is the root\n",
    "\n",
    "# Command to check the files inside root hdfs directory\n",
    "hadoop fs -ls /\n",
    "\n",
    "# Command to create directory in hdfs\n",
    "hadoop fs -mkdir /input_data\n",
    "\n",
    "\n",
    "# Copy data from local file system to Hdfs\n",
    "hadoop fs -put test_demo/trees.csv /input_data\n",
    "\n",
    "# Copy from HDFS path to local file system\n",
    "hadoop fs -copyToLocal /input_data/trees.csv ./\n",
    "\n",
    "# Command to execute map reduce code\n",
    "hadoop jar hadoop-streaming-2.4.0.jar -files mapper.py,reducer.py -mapper mapper.py -reducer reducer.py -input /test/demo.txt -output /output\n",
    "\n",
    "\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
